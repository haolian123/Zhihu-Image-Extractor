## 项目介绍
​		这是一个用于从知乎问题页面提取和下载图片的Python脚本，因为最近刚好一直在找壁纸，所以就顺便写一下这个脚本来帮助下载图片。使用`requests`和`re`库来获取页面内容，并提取出所有符合特定模式的图片URL。然后下载这些图片并保存到指定的目录。

### 功能：

- 提取指定知乎问题页面上的所有图片URL。
- 将找到的图片下载到本地目录。
- 限制下载的图片数量。
- 在下载过程中设置延时，以减少对服务器的负担。

## 使用方法
### 安装依赖
确保Python环境中安装了`requests`库。可以通过以下命令进行安装：
```bash
pip install requests
```

### 使用
首先，将`Extracter`类和所需的库导入Python文件中。用法示例：

```python
from Extracter import Extracter

# 设置想要提取图片的知乎问题页面URL
url = "https://www.zhihu.com/question/您的问题ID"
# 设置保存图片的路径
savePath = "图片"
# 设置最多下载多少图片
maxNum = 20
# 调用extractImages方法提取并下载图片
Extracter.extractImages(url, savePath,maxNum)
```

### 参数说明
- `url`: 想要提取图片的知乎问题页面的URL。
- `savePath`: 图片下载后保存的本地路径。
- `maxNum` (可选): 最大下载图片数量，默认为20。

## 注意事项
- 确保遵守知乎的使用条款和网络爬虫规则。
- 不要过度使用此工具，以免对知乎服务器造成过大负担。
- 该工具仅用于学习和研究目的，请勿用于任何商业用途。

## 不足

- **单线程执行**：当前版本的工具使用单线程执行下载任务，这可能导致在处理大量图片时速度较慢。
- **没有异常检测和错误恢复机制**：如果下载过程中出现任何错误（如网络问题），工具不会自动重试或跳过失败的图片。

由于想快点实现功能，写得比较粗糙，后面有空会随缘优化~